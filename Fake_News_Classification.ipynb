{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake News Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9lAgFJaQElp"
      },
      "source": [
        "# **Creating a Fake News Classification System**\n",
        "A Comparison of Naive Bayes and Smoothed Unigram-Bigram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmMnc5FjJlb9"
      },
      "source": [
        "Real news example:\n",
        "```\n",
        "The OpenAI technology, known as GPT-2, is designed to predict the next word given all the previous words it is shown within some text. The language-based model has been trained on a dataset of 8 million web pages.\n",
        "```\n",
        "\n",
        "Fake news example:\n",
        "```\n",
        "In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bwd0ftkKJpiE"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAGaFGZHJv7T"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1hWZSKPKVdW"
      },
      "source": [
        "**PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAwOnFlmJwbs",
        "outputId": "e84d6c8e-ff61-4195-c5a8-4f2d5efd67fb"
      },
      "source": [
        "import os\n",
        "import io\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "root_path = os.path.join(os.getcwd(), \"drive\", \"My Drive/Colab Notebooks\") # replace based on your Google drive organization\n",
        "dataset_path = os.path.join(root_path, \"4740_p1_dataset\") # same here\n",
        "\n",
        "with io.open(os.path.join(dataset_path, \"trueDataTrain.txt\"), encoding='utf8') as real_file:\n",
        "  real_news = real_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"trueDataValidation.txt\"), encoding='utf8') as real_file:\n",
        "  real_news_validation = real_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"fakeDataTrain.txt\"), encoding='utf8') as fake_file:\n",
        "  fake_news = fake_file.read()\n",
        "with io.open(os.path.join(dataset_path, \"fakeDataValidation.txt\"), encoding='utf8') as real_file:\n",
        "  fake_news_validation = real_file.read()\n",
        "\n",
        "# Convert all words to lowercase and tokenize (split sentences around spaces and punctuation). \n",
        "# Need two datasets: one for training the model, and another for validation. \n",
        "tokenized_real_news_training = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(real_news)]\n",
        "tokenized_fake_news_training = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(fake_news)]\n",
        "tokenized_real_news_validation = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(real_news_validation)]\n",
        "tokenized_fake_news_validation = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(fake_news_validation)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_fmnD5NKrgM"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZttQ_t7J2mq",
        "outputId": "c7b09fa4-24bd-4163-be83-48463eefce3a"
      },
      "source": [
        "print(\"Sanity check: \")\n",
        "print(tokenized_real_news_training[0])\n",
        "print(tokenized_fake_news_training[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sanity check\n",
            "['london', '(', 'reuters', ')', '-', 'britain', 'is', 'seeking', 'to', 'build', 'on', 'the', 'recent', 'momentum', 'in', 'the', 'brexit', 'divorce', 'talks', 'with', 'the', 'european', 'union', 'before', 'a', 'summit', 'next', 'month', ',', 'a', 'spokeswoman', 'for', 'britain', 's', 'department', 'for', 'exiting', 'the', 'european', 'union', 'said', 'on', 'tuesday', '.']\n",
            "['rick', 'santorum', 'had', 'the', 'impossible', 'job', 'of', 'defending', 'donald', 'trump', 'during', 'friday', 's', 'real', 'time', 'and', 'he', 'found', 'himself', 'humiliatingly', 'outnumbered.bill', 'maher', 'began', 'by', 'expressing', 'astonishment', 'at', 'how', 'the', 'republican', 'nominee', 'has', 'been', 'acting', 'throughout', 'the', 'campaign', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwH-yejtKFmD"
      },
      "source": [
        "# Function to insert <s> (sentence markers) at the beginning of sentences.\n",
        "def insert_sentence_markers(tokenized_news): \n",
        "  for sentence in tokenized_news: \n",
        "    for i, word in enumerate(sentence): \n",
        "      if i is 0: \n",
        "        sentence.insert(0, '<s>')\n",
        "      if word == '.' and i != len(sentence)-1:\n",
        "        sentence.insert(i+1, '</s>')\n",
        "        sentence.insert(i+2, '<s>')\n",
        "    sentence.append('</s>')\n",
        "  return tokenized_news"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbaA6TF5KPLQ"
      },
      "source": [
        "tokenized_fake_news_training = insert_sentence_markers(tokenized_fake_news_training)\n",
        "tokenized_fake_news_validation = insert_sentence_markers(tokenized_fake_news_validation)\n",
        "tokenized_real_news_training = insert_sentence_markers(tokenized_real_news_training)\n",
        "tokenized_real_news_validation = insert_sentence_markers(tokenized_real_news_validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AUOkJuGsKmqN"
      },
      "source": [
        "# Creating lists of words in fake news and real news datasets (fake versus real vocabulary).\n",
        "fake_words = [word for sentence in tokenized_fake_news_training for word in sentence]\n",
        "real_words = [word for sentence in tokenized_real_news_training for word in sentence]\n",
        "\n",
        "# Unigram = singular words in datasets.\n",
        "unigram_dictionary = set()\n",
        "for word in fake_words: \n",
        "  unigram_dictionary.add(word)\n",
        "for word in real_words: \n",
        "  unigram_dictionary.add(word)\n",
        "\n",
        "# Bigrams = pairs of words in datasets.\n",
        "bigram_dictionary = set()\n",
        "for sentence in tokenized_fake_news_training:\n",
        "  for i, word in enumerate(sentence):\n",
        "    if i is not 0:\n",
        "      bigram_dictionary.add((sentence[i-1], word))\n",
        "\n",
        "for sentence in tokenized_real_news_training:\n",
        "  for i, word in enumerate(sentence):\n",
        "    if i is not 0:\n",
        "      bigram_dictionary.add((sentence[i-1], word))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNyO4TndKoIu"
      },
      "source": [
        "# Counting up words in datasets.\n",
        "fake_word_counts = np.zeros(len(unigram_dictionary))\n",
        "real_word_counts = np.zeros(len(unigram_dictionary))\n",
        "\n",
        "# Mapping words to indices (for efficiency in later processing).\n",
        "word_to_idx = {}\n",
        "idx_to_word = {}\n",
        "for i, word in enumerate(unigram_dictionary):\n",
        "  word_to_idx[word] = i\n",
        "  idx_to_word[i] = word\n",
        "\n",
        "for word in fake_words:\n",
        "  fake_word_counts[word_to_idx[word]] += 1\n",
        "for word in real_words: \n",
        "  real_word_counts[word_to_idx[word]] += 1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd37KaMsKuws"
      },
      "source": [
        "# Counting up bigrams in datasets, and mapping to indices. \n",
        "fake_bigram_counts = np.zeros(len(bigram_dictionary))\n",
        "real_bigram_counts = np.zeros(len(bigram_dictionary))\n",
        "\n",
        "word_to_bigram_idx = {}\n",
        "idx_to_bigram_word = {}\n",
        "for i, pair in enumerate(bigram_dictionary):\n",
        "  word_to_bigram_idx[pair] = i\n",
        "  idx_to_bigram_word[i] = pair\n",
        "\n",
        "for sentence in tokenized_fake_news_training:\n",
        "  for i, word in enumerate(sentence):\n",
        "    if i is not 0:\n",
        "      fake_bigram_counts[word_to_bigram_idx[(sentence[i-1], word)]] += 1\n",
        "for sentence in tokenized_real_news_training:\n",
        "  for i, word in enumerate(sentence):\n",
        "    if i is not 0:\n",
        "      real_bigram_counts[word_to_bigram_idx[(sentence[i-1], word)]] += 1"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3eLC5wF2Kwpg"
      },
      "source": [
        "# Setting up unigram and bigram real and fake dictionaries to be used later.\n",
        "\n",
        "unigram_fake_dict = {}\n",
        "unigram_real_dict = {}\n",
        "bigram_real_dict = {}\n",
        "bigram_fake_dict = {}\n",
        "\n",
        "\n",
        "for i, count in enumerate(fake_word_counts): \n",
        "  unigram_fake_dict[idx_to_word[i]] = count\n",
        "\n",
        "for i, count in enumerate(real_word_counts): \n",
        "  unigram_real_dict[idx_to_word[i]] = count\n",
        "\n",
        "for i, count in enumerate(fake_bigram_counts): \n",
        "  bigram_fake_dict[idx_to_bigram_word[i]] = count\n",
        "\n",
        "for i, count in enumerate(real_bigram_counts): \n",
        "  bigram_real_dict[idx_to_bigram_word[i]] = count"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYFWAanXLlMb"
      },
      "source": [
        "**UNSMOOTHED LANGUAGE MODELS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqYSCwomLsdI"
      },
      "source": [
        "Unigram Model (Singular Words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_CblUhFLo92"
      },
      "source": [
        "# Computes the probabilities for a unigram model\n",
        "def unsmoothed_unigram(lst):\n",
        "  dictionary = {}\n",
        "  total_fake = np.sum(fake_word_counts)\n",
        "  total_real = np.sum(real_word_counts)\n",
        "  for word in lst: \n",
        "    fake_uni = fake_word_counts[word_to_idx[word]] / total_fake\n",
        "    real_uni = real_word_counts[word_to_idx[word]] / total_real\n",
        "    dictionary[word] = (fake_uni, real_uni)\n",
        "  return dictionary"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oC_gAvH4L39r",
        "outputId": "d61cde08-222c-44eb-9f72-7e574139f457"
      },
      "source": [
        "# unsmoothed_unigram is now a dictionary that stores words and maps them to their probabilities in the real and fake news datasets. \n",
        "print(unsmoothed_unigram(['donald', 'trump', 'the']))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'donald': (0.001574398869408056, 0.001121394894214776), 'trump': (0.0067686754543055, 0.005881439615528737), 'the': (0.04798429940025114, 0.051731178201232464)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaZEu8zbL6Bh"
      },
      "source": [
        "Bigram Model (Pairs of Words)\n",
        "\n",
        "$p(w_n\\mid w_{n-1})=\\frac{C(w_{n-1}w_n)}{C(w_{n-1})}$\n",
        "- Want to find the probability of a word given the previous word based on counts in the current training datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1U6PZ1bvL8ht"
      },
      "source": [
        "def unsmoothed_bigram(lst):\n",
        "  dictionary = {}\n",
        "  total_fake = np.sum(fake_bigram_counts)\n",
        "  total_real = np.sum(real_bigram_counts)\n",
        "  for word in lst: \n",
        "    fake_uni = fake_bigram_counts[word_to_bigram_idx[word]] / fake_word_counts[word_to_idx[word[0]]]\n",
        "    real_uni = real_bigram_counts[word_to_bigram_idx[word]] / real_word_counts[word_to_idx[word[0]]]\n",
        "    dictionary[word] = (fake_uni, real_uni)\n",
        "  return dictionary"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtTfwgKpMAI1",
        "outputId": "ccd1a17a-67f5-45c5-e1ed-4aa191120ecb"
      },
      "source": [
        "# unsmoothed_bigram is similar to unigram above but for pairs of words (to map succession).\n",
        "print(unsmoothed_bigram([('the','donald'), ('donald','trump')]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('the', 'donald'): (0.0014047892008848557, 7.385306194794836e-05), ('donald', 'trump'): (0.770751312335958, 0.9648405560098119)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOXZZd-5M3rD"
      },
      "source": [
        "**SMOOTHED LANGUAGE MODEL**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfInjNtcM9B_"
      },
      "source": [
        "Step 1: Unknown Word Handling\n",
        "\n",
        "Handling unknown words by converting tokens that are not in the \"top k\" most common words to '<UNK>' during pre-processing. k = 4/5 of the total length of word tokens in the trainint set, so k is dependent on the size of the training dataset. Decided on this value for k by playing around with various inputs and corresponding perplexities in an attempt to optimize perplexity. Next, need to look at the sorted word counts that we determined above, remove the bottom 1/5, and replace those with '<UNK>'. \n",
        "\n",
        "Also used the validation testing data below to determine perplexities with various k values in order to optimize which one we were choosing.\n",
        "\n",
        "This step is crucial for the smoothed language model in order to handle any unknown words that the model may come across in testing/future data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUI-eWEBM6iF"
      },
      "source": [
        "def handle_unk(word_counts):\n",
        "  k = int(4 * len(word_counts) / 5) #DESIGN DECISION: choose k val for least common words and convert to <UNK>\n",
        "  items = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "  topk = dict(list(items)[:k])\n",
        "  bottomk = dict(list(items)[k:])\n",
        "  \n",
        "  len_unk = len(bottomk)\n",
        "  topk[\"<UNK>\"] = len_unk\n",
        "  return topk"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZ9MAvjDOX5L",
        "outputId": "8c2263a1-1438-405d-cd1b-643b26a1078f"
      },
      "source": [
        "# Test:\n",
        "print(handle_unk({\"to\": 40, \"and\": 39, \"tree\": 12, \"sun\": 3, \"flower\": 3, \"mine\": 1, \"the\": 50, \"a\": 40}))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'the': 50, 'to': 40, 'a': 40, 'and': 39, 'tree': 12, 'sun': 3, '<UNK>': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVimdWKPOo3K"
      },
      "source": [
        "unigram_fake_dict = handle_unk(unigram_fake_dict)\n",
        "unigram_real_dict = handle_unk(unigram_real_dict)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHBLBoLgOql7"
      },
      "source": [
        "def handle_unk_bigrams(tokenized_training, unigram_dic):\n",
        "  dic = {}\n",
        "  for sentence in tokenized_training:\n",
        "    for i, word in enumerate(sentence):\n",
        "      if i is not 0:\n",
        "        if sentence[i-1] in unigram_dic: \n",
        "          pair1 = sentence[i-1]\n",
        "        else: \n",
        "          pair1 = '<UNK>'\n",
        "          \n",
        "        if word in unigram_dic: \n",
        "          pair2 = word\n",
        "        else: \n",
        "          pair2 = '<UNK>'\n",
        "      \n",
        "        if (pair1, pair2) in dic: \n",
        "          dic[(pair1, pair2)] = dic[(pair1, pair2)] +1\n",
        "        else: \n",
        "          dic[(pair1, pair2)] = 1\n",
        "  return dic"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npHL3VmiOsWL"
      },
      "source": [
        "bigram_real_dict = handle_unk_bigrams(tokenized_real_news_training, unigram_real_dict)\n",
        "bigram_fake_dict = handle_unk_bigrams(tokenized_fake_news_training, unigram_fake_dict)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3EBQecjOubx"
      },
      "source": [
        "Step 2: Add-K Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vYBZOE3OyOe"
      },
      "source": [
        "def add_k_unigram(dic, k):\n",
        "  s = sum(dic.values())\n",
        "  new_dic = {}\n",
        "  for word in dic: \n",
        "    new_dic[word] = (dic[word] + k) / (s + k*len(dic))\n",
        "  return new_dic "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiv4ZOZdO2BL"
      },
      "source": [
        "def add_k_bigram(uni_dic, bi_dic, k):\n",
        "  new_dic = {}\n",
        "  for pair1, pair2 in bi_dic: \n",
        "    new_dic[(pair1, pair2)] = (bi_dic[(pair1, pair2)] + k) / (uni_dic[pair1] + k*len(uni_dic))\n",
        "  return new_dic"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4oj3BO_O4Or"
      },
      "source": [
        "unigram_fake_probs = add_k_unigram(unigram_fake_dict, .1)\n",
        "unigram_real_probs = add_k_unigram(unigram_real_dict, .1)\n",
        "bigram_real_probs = add_k_bigram(unigram_real_dict, bigram_real_dict, .1)\n",
        "bigram_fake_probs = add_k_bigram(unigram_fake_dict, bigram_fake_dict, .1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8EuiOXAO7yQ"
      },
      "source": [
        "Step 3: Computing Perplexity\n",
        "\n",
        "Smoothing the data changes all of the probabilities slightly in order to shift some of the probability mass from frequent terms to unknowns and 0-values. Need to compute perplexity after smoothing because this change can alter each probability, and therefore change perplexity.\n",
        "\n",
        "Computing perplexity as follows:\n",
        "\\begin{align*}\n",
        "PP &= \\left(\\prod_i^N\\frac{1}{P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)}\\right)^{\\frac{1}{N}}\\\\\n",
        "&=\\exp \\frac{1}{N}\\sum_{i}^N-\\log P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)\n",
        "\\end{align*}\n",
        "where $N$ is the total number of tokens in the test corpus and $P\\left(W_i\\mid W_{i-1}, ...W_{i-n+1}\\right)$\n",
        "is the n-gram probability of your model. Under the second definition above, perplexity\n",
        "is a function of the average (per-word) log probability: use this to avoid numerical\n",
        "computation errors.\n",
        "\n",
        "In this case, lower perplexity indicates a better model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRaOioHpPF4k"
      },
      "source": [
        "def calculate_unigram_perp(uni_dic, validation_set):\n",
        "  words = [word for sentence in validation_set for word in sentence]\n",
        "  sum = 0.0\n",
        "  for word in words: \n",
        "    if word in uni_dic:\n",
        "      sum += - np.log(uni_dic[word])\n",
        "    else: \n",
        "      sum += - np.log(uni_dic['<UNK>'])\n",
        "  return np.exp(sum / len(words))"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym0VzLqEPGVj",
        "outputId": "02c6ed0c-cc07-45ad-8be0-810075f95a00"
      },
      "source": [
        "uni_perp_fake = calculate_unigram_perp(unigram_fake_probs, tokenized_fake_news_validation)\n",
        "uni_perp_fr = calculate_unigram_perp(unigram_fake_probs, tokenized_real_news_validation)\n",
        "uni_perp_real = calculate_unigram_perp(unigram_real_probs, tokenized_real_news_validation)\n",
        "uni_perp_rf = calculate_unigram_perp(unigram_real_probs, tokenized_fake_news_validation)\n",
        "\n",
        "print(uni_perp_fake)\n",
        "print(uni_perp_fr)\n",
        "print(uni_perp_real)\n",
        "print(uni_perp_rf)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1188.5861954910306\n",
            "1234.9475581317192\n",
            "1038.6563391108798\n",
            "1542.885797644777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbMiacTlPJqy"
      },
      "source": [
        "def calculate_bigram_perp(uni_dic, bi_dic, validation_set):\n",
        "  words = [word for sentence in validation_set for word in sentence]\n",
        "  sum = 0\n",
        "  for i, word in enumerate(words): \n",
        "    if i is not 0:\n",
        "      if (words[i-1], word) in bi_dic:\n",
        "        sum += - np.log(bi_dic[(words[i-1], word)])\n",
        "      else: \n",
        "        if words[i-1] not in uni_dic and word not in uni_dic: \n",
        "          if ('<UNK>', '<UNK>') in bi_dic:\n",
        "            sum += - np.log(bi_dic[('<UNK>', '<UNK>')])\n",
        "        elif words[i-1] not in uni_dic: \n",
        "          if ('<UNK>', word) in bi_dic:\n",
        "            sum += - np.log(bi_dic[('<UNK>', word)])\n",
        "        else: \n",
        "          if (words[i-1], '<UNK>') in bi_dic: \n",
        "            sum += - np.log(bi_dic[(words[i-1], '<UNK>')])\n",
        "  return np.exp(sum / len(words))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ykxm31LPLD-",
        "outputId": "a2734778-e6f2-4574-ef3a-8fa1e7e8ee90"
      },
      "source": [
        "bi_perp_fake = calculate_bigram_perp(unigram_fake_probs, bigram_fake_probs, tokenized_fake_news_validation)\n",
        "bi_perp_fr = calculate_bigram_perp(unigram_fake_probs, bigram_fake_probs, tokenized_real_news_validation)\n",
        "bi_perp_real = calculate_bigram_perp(unigram_real_probs, bigram_real_probs, tokenized_real_news_validation)\n",
        "bi_perp_rf = calculate_bigram_perp(unigram_real_probs, bigram_real_probs, tokenized_fake_news_validation)\n",
        "\n",
        "print(bi_perp_fake)\n",
        "print(bi_perp_fr)\n",
        "print(bi_perp_real)\n",
        "print(bi_perp_rf)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "335.06906565144646\n",
            "217.64542651350186\n",
            "186.6780270411643\n",
            "109.45830115341417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nWkNG8WPVvM"
      },
      "source": [
        "**ALTERNATIVE METHOD: MULTINOMIAL NAIVE BAYES**\n",
        "\n",
        "Suppose we have a news article *d* and its label *c* (either 0 or 1).\n",
        "\\begin{align*}\n",
        "P(c|d)=\\frac{P(d|c)P(c)}{P(d)}\n",
        "\\end{align*}\n",
        "Likelihood: $P(d|c)$. In real/fake corpus, how likely *d* would appear.\n",
        "\n",
        "Prior: $P(c)$. The probability of real/fake news in general.\n",
        "\n",
        "Posterior: $P(c|d)$. Given *d*, how likely is it that it is real/fake.\n",
        "\n",
        "Goal: $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(c|d)$, which is equivalent to $\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)$.\n",
        "\n",
        "The equivalence holds because $P(d)$ is the same for any $c$. Thus the denominator can be dropped.\n",
        "\n",
        "Denote $d=\\{x_1, x_2, ..., x_n\\}$ where $x_i$'s are words in the news *d* (sometimes called features). Unlike n-gram language modelling, we make the multinomial Naive Bayes independence assumption here, where we assume positions of words do not matter. Formally, \n",
        "\\begin{align*}\n",
        "&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(d|c)P(c)\\\\\n",
        "=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1, ..., x_n|c)P(c)\\\\\n",
        "=&\\underset{c\\in \\{0,1\\}}{\\operatorname{argmax}} P(x_1|c)P(x_2|c)...P(x_n|c)\n",
        "\\end{align*}\n",
        "\n",
        "Now only need to collect the occurences of each word for the classification. This bag-of-words feature is a dictionary that maps word to its occurrences, without considering order. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHwbIgnoPYwV"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import numpy as np\n",
        "\n",
        "labels = []\n",
        "\n",
        "# Get all sentences from fake and real, and put each in 2d array with real or fake tag.\n",
        "fake_sentences = nltk.sent_tokenize(fake_news)\n",
        "real_sentences = nltk.sent_tokenize(real_news)\n",
        "\n",
        "for sent in fake_sentences:\n",
        "  labels.append(0)\n",
        "for sent in real_sentences:\n",
        "  labels.append(1)\n",
        "\n",
        "sentences = fake_sentences + real_sentences\n",
        "\n",
        "# Convert text data to probability value vectors.\n",
        "vec2 = CountVectorizer()\n",
        "X = vec2.fit_transform(sentences)\n",
        "\n",
        "classification = MultinomialNB()\n",
        "classification.fit(X, labels)\n",
        "\n",
        "# Test on validation data.\n",
        "fake_test = nltk.sent_tokenize(fake_news_validation)\n",
        "real_test = nltk.sent_tokenize(real_news_validation)\n",
        "test = fake_test + real_test\n",
        "\n",
        "predicts = classification.predict(vec2.transform(test))"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}